{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3281659-b1ec-4a9a-9a59-f8efc158d47f",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a63f7b9-b217-48ff-a665-6d82327e0d09",
   "metadata": {
    "tags": []
   },
   "source": [
    "## GPU enabled Kubeflow notebook\n",
    "In this notebook we simulate training a model across multiple compute nodes, each of which using multiple GPUs with the tensorflow framework. We demonstrate this with a convolutional neural network for image classification trained on cifar10 dataset. \n",
    "\n",
    "This example was tested on: \n",
    "* [charmed kubeflow](https://charmed-kubeflow.io/)\n",
    "* in a [MicroK8s](https://microk8s.io/) kubernetes cluster\n",
    "* deployed across multiple NVIDIA DGX nodes, each with 8GPUs. \n",
    "* using drivers: NVIDIA-SMI 515.48.07    Driver Version: 515.48.07    CUDA Version: 11.7\n",
    "* using `tensorflow-gpu==2.10.0` (see `requirements.txt` for other dependencies)\n",
    "\n",
    "This example was executed from a Kubeflow Notebook Server with 4CPU, 16GB RAM, and 8GPUs, but it could run with as little as 2GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2766fd-1a7d-439b-9144-920f481b506b",
   "metadata": {},
   "source": [
    "This is a local demonstration of multi-node, multi-gpu distributed training.  This demonstration is executed by running two separate instances of the same training process and setting them up to coordinate as if they were on separate compute nodes.  Typically these would be executed on different machines and communicate remotely.\n",
    "\n",
    "This demo is a modified version of the [Tensorflow Multi-worker training with Keras](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras) demo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2add865-30f8-45f5-a5d7-38928e0ac353",
   "metadata": {},
   "source": [
    "# Setup and Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4afaffef-d615-42f6-a1db-afdb5f998b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-24 14:35:24.130201: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-24 14:35:24.283509: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-02-24 14:35:25.040054: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvrtc.so.11.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-02-24 14:35:25.040215: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvrtc.so.11.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-02-24 14:35:25.040224: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/opt/conda/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from gpu_distributed_utilities import build_and_compile_cnn_model, get_cifar10_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a7604f-2aa4-4828-94b8-2ff9e0cf82f4",
   "metadata": {},
   "source": [
    "Distributed training is configured via the `TF_CONFIG` environment variable.  We clear it here and will explicitly set it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de8782a5-3aba-4f6e-8063-50fd3855bc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ.pop('TF_CONFIG', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad3f02f-454b-4ee0-89ff-f56426ad1d16",
   "metadata": {},
   "source": [
    "To simulate multi-node training, each instance will only be shown a subset of the GPUs on this node.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fcdf2f6-7c70-4c7f-8b76-3bbf419f2dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_VISIBLE_DEVICES = {\n",
    "    0: \"0,1\",\n",
    "    1: \"6,7\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a655dd6e-5016-4ff2-96c8-557d6f0e0dab",
   "metadata": {},
   "source": [
    "Helpers for setting up the separate node environments below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea6df6f6-622c-4f1e-8cd8-a9d240db629a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emit_tf_config(role_id):\n",
    "    tf_config = {\n",
    "    'cluster': {\n",
    "        # This defines two workers in the group, each reachable on localhost\n",
    "        'worker': ['localhost:12345', 'localhost:23456']\n",
    "    },\n",
    "        # This defines the type and index of this particular worker\n",
    "        'task': {'type': 'worker', 'index': role_id}\n",
    "    }\n",
    "    os.environ['TF_CONFIG'] = json.dumps(tf_config)\n",
    "\n",
    "def emit_visible_devices(role_id):\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=GPU_VISIBLE_DEVICES[role_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be12d847-b6cd-4025-a9f3-8607e10168e7",
   "metadata": {},
   "source": [
    "Kill any previous background processes, just in case..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a70c138-5ca8-4cfd-9f71-88298fbf51b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All background processes were killed.\n"
     ]
    }
   ],
   "source": [
    "%killbgscripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db38c20-819b-424e-b973-60daf26ba796",
   "metadata": {},
   "source": [
    "# Deploy the training workload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849650b3-88fd-4de8-bf0b-9e8b4901e323",
   "metadata": {},
   "source": [
    "Below we deploy two workers, both defined by the included script `gpu_distributed_main.py`.  That script essentially just does:\n",
    "\n",
    "```python\n",
    "def main(...):\n",
    "    # Get some data\n",
    "    X_train_scaled, y_train_encoded, X_test_scaled, y_test_encoded = get_cifar10_dataset()\n",
    "\n",
    "    # Set up a CNN model using the tf.distribute.MultiWorkerMirroredStrategy() scope, which tells the \n",
    "    # model to coordinate with other nodes\n",
    "    strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "    with strategy.scope():\n",
    "        model = build_and_compile_cnn_model()\n",
    "\n",
    "    # Fit the model, as per some settings\n",
    "    model.fit(x=X_train_scaled, y=y_train_encoded, epochs=epochs, batch_size=batch_size)\n",
    "```\n",
    "\n",
    "Where we import model setup from `gpu_distributed_utilities.py`.  \n",
    "\n",
    "Executing `gpu_distributed_worker.py` results in a worker that has data, knows how to coordinate with other workers (inferred from the `TF_CONFIG` environment variable, as shown below), and is waiting for everyone to be ready.  Once all workers are online, the training will begin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94618c57-c9a5-421c-9788-dd983f5b9ad4",
   "metadata": {},
   "source": [
    "## Start the chief worker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472c85b3-b57e-47f6-a347-6c678eeea4f5",
   "metadata": {},
   "source": [
    "The 0th worker is the chief, and often takes on additional tasks like coordination between workers or saving checkpoints.  In our case, the chief has no additional roles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60611a3b-8571-436f-b0f0-b530f8f28a8d",
   "metadata": {},
   "source": [
    "Set environment variables for the TFConfig and visible devices, so we can launch a process that has that specific configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e38d97a-1b18-4ebd-8e8c-36c23c50b793",
   "metadata": {},
   "outputs": [],
   "source": [
    "role_id = 0  # Chief\n",
    "\n",
    "emit_tf_config(role_id)\n",
    "emit_visible_devices(role_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606eb412-fdbd-42d6-81c1-e1cb83c58b07",
   "metadata": {},
   "source": [
    "And we can confirm these were set properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba90e5a1-f92b-4eca-805b-55d6fe927a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES = 0,1\n",
      "TF_CONFIG = {\"cluster\": {\"worker\": [\"localhost:12345\", \"localhost:23456\"]}, \"task\": {\"type\": \"worker\", \"index\": 0}}\n"
     ]
    }
   ],
   "source": [
    "print(f\"CUDA_VISIBLE_DEVICES = {os.environ['CUDA_VISIBLE_DEVICES']}\")\n",
    "print(f\"TF_CONFIG = {os.environ['TF_CONFIG']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69899f9-5f46-407a-b93f-eb9bbb671587",
   "metadata": {},
   "source": [
    "Now execute this chief worker in the background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d2c82c3-10aa-42aa-b4c5-597df1920242",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --bg -s \"$role_id\"\n",
    "echo working on job_$1\n",
    "python gpu_distributed_worker.py --batch-size=256 --epochs 10 &> job_$1.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8989b303-1d72-455b-a07e-e77221b0c985",
   "metadata": {},
   "source": [
    "And if we `tail -f job_0.log`, we should see something like:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cbc8dd-e65e-4829-8071-00b67ec0cf25",
   "metadata": {},
   "source": [
    "```\n",
    "tail -f job_0.log\n",
    "\n",
    "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
    "2023-02-23 22:02:00.803116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38406 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:07:00.0, compute capability: 8.0\n",
    "2023-02-23 22:02:00.805756: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 38406 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:0f:00.0, compute capability: 8.0\n",
    "2023-02-23 22:02:00.841467: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:worker/replica:0/task:0/device:GPU:0 with 38406 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:07:00.0, compute capability: 8.0\n",
    "2023-02-23 22:02:00.843643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:worker/replica:0/task:0/device:GPU:1 with 38406 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:0f:00.0, compute capability: 8.0\n",
    "2023-02-23 22:02:00.879935: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12345, 1 -> localhost:23456}\n",
    "2023-02-23 22:02:00.880131: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12345, 1 -> localhost:23456}\n",
    "2023-02-23 22:02:00.881226: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345\n",
    "2023-02-23 22:02:00.884138: I tensorflow/core/distributed_runtime/coordination/coordination_service.cc:526] /job:worker/replica:0/task:0 has connected to coordination service. Incarnation: 4418798052974762676\n",
    "2023-02-23 22:02:00.890014: I tensorflow/core/distributed_runtime/coordination/coordination_service_agent.cc:281] Coordination agent has successfully connected.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a42f746-3758-4053-ae52-6b79b0f8c65c",
   "metadata": {},
   "source": [
    "where we see the worker is ready and waiting for others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e633a365-6872-4cf4-8ec9-c43c45ae81d3",
   "metadata": {},
   "source": [
    "## Start the additional worker(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d23696-0d35-476d-8bd4-193c0e46037e",
   "metadata": {},
   "source": [
    "We repeat the same task as above, except we pass a different `role_id` to the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c897984d-04d6-4a0e-8956-0e7a03407fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "role_id = 1  # Worker\n",
    "\n",
    "emit_tf_config(role_id)\n",
    "emit_visible_devices(role_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b81a39-8f2e-4e25-9225-1f2867d20425",
   "metadata": {},
   "source": [
    "And we can confirm these were set properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84f8822b-8e88-4bc7-a0bb-abf893e36deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES = 6,7\n",
      "TF_CONFIG = {\"cluster\": {\"worker\": [\"localhost:12345\", \"localhost:23456\"]}, \"task\": {\"type\": \"worker\", \"index\": 1}}\n"
     ]
    }
   ],
   "source": [
    "print(f\"CUDA_VISIBLE_DEVICES = {os.environ['CUDA_VISIBLE_DEVICES']}\")\n",
    "print(f\"TF_CONFIG = {os.environ['TF_CONFIG']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef65a67-397a-4e46-b5b7-0003175c9976",
   "metadata": {},
   "source": [
    "Where we see the worker index of 1 and the updated visible CUDA devices.  \n",
    "\n",
    "Now execute this worker in the background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2163d476-84a8-4d1f-b739-a3737a256dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --bg -s \"$role_id\"\n",
    "echo working on job_$1\n",
    "python gpu_distributed_worker.py --batch-size=256 --epochs 10 &> job_$1.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ddb0f2-0dd0-49d0-89e3-f44f0d7d8e32",
   "metadata": {},
   "source": [
    "And if we `tail -f job_1.log`, we should see this second worker set up like the first, and then see both workers start training the model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b871d9c1-51eb-47bf-abe8-46dfea6edbb2",
   "metadata": {},
   "source": [
    "With training running, we should also be able to see activity in the GPUs using the `nvidia-smi` tool.  For example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ae994d-515a-4634-b891-b27a39ae32ca",
   "metadata": {},
   "source": [
    "```\n",
    "+-----------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 470.141.03   Driver Version: 470.141.03   CUDA Version: 11.4     |\n",
    "|-------------------------------+----------------------+----------------------+\n",
    "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                               |                      |               MIG M. |\n",
    "|===============================+======================+======================|\n",
    "|   0  NVIDIA A100-SXM...  On   | 00000000:07:00.0 Off |                    0 |\n",
    "| N/A   30C    P0    79W / 400W |  39872MiB / 40536MiB |      8%      Default |\n",
    "|                               |                      |             Disabled |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "|   1  NVIDIA A100-SXM...  On   | 00000000:0F:00.0 Off |                    0 |\n",
    "| N/A   30C    P0    68W / 400W |  39872MiB / 40536MiB |      6%      Default |\n",
    "|                               |                      |             Disabled |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "|   2  NVIDIA A100-SXM...  On   | 00000000:47:00.0 Off |                    0 |\n",
    "| N/A   29C    P0    55W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
    "|                               |                      |             Disabled |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "|   3  NVIDIA A100-SXM...  On   | 00000000:4E:00.0 Off |                    0 |\n",
    "| N/A   29C    P0    51W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
    "|                               |                      |             Disabled |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "|   4  NVIDIA A100-SXM...  On   | 00000000:87:00.0 Off |                    0 |\n",
    "| N/A   33C    P0    53W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
    "|                               |                      |             Disabled |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "|   5  NVIDIA A100-SXM...  On   | 00000000:90:00.0 Off |                    0 |\n",
    "| N/A   32C    P0    55W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
    "|                               |                      |             Disabled |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "|   6  NVIDIA A100-SXM...  On   | 00000000:B7:00.0 Off |                    0 |\n",
    "| N/A   34C    P0    82W / 400W |  39872MiB / 40536MiB |      7%      Default |\n",
    "|                               |                      |             Disabled |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "|   7  NVIDIA A100-SXM...  On   | 00000000:BD:00.0 Off |                    0 |\n",
    "| N/A   37C    P0    75W / 400W |  39872MiB / 40536MiB |      7%      Default |\n",
    "|                               |                      |             Disabled |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4917051a-154b-483f-9e57-8cafc0719107",
   "metadata": {},
   "source": [
    "where we see GPUs 0, 1, 6, and 7 are working."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42d3cc6-8bab-4c33-9a28-283902b211d1",
   "metadata": {},
   "source": [
    "This demonstrates how we can coordinate multiple processes using GPUs to train the same model.  To extend this to multiple nodes, all we need to do is update the `TF_CONFIG` environment variables to add workers from different machines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
